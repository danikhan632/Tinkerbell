# vLLM Backend Configuration
# Copy this file to .env and configure as needed

# Enable vLLM backend for sampling
USE_VLLM=true

# Enable Megatron backend for training (optional)
USE_MEGATRON=false

# vLLM Server Connection
# Option 1: Use base URL
VLLM_BASE_URL=http://localhost:8000

# Option 2: Use host and port separately
# VLLM_HOST=0.0.0.0
# VLLM_PORT=8000

# vLLM Configuration
VLLM_GROUP_PORT=51216          # Port for weight update group
VLLM_TIMEOUT=5.0               # Connection timeout in seconds
VLLM_ENABLE_LORA=true          # Enable LoRA adapter support
VLLM_MAX_LORAS=4               # Maximum concurrent LoRA adapters
VLLM_MAX_LORA_RANK=64          # Maximum LoRA rank

# Worker Configuration
MAX_WORKERS=4                   # Number of worker threads

# Megatron Configuration (if USE_MEGATRON=true)
# MEGATRON_SERVER_URL=http://localhost:5000

# Flask Server Configuration
PORT=8000                       # Port for the Flask API server
