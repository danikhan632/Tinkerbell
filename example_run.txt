======================================================================
Tinkerbell: Async RL Training with vLLM Policy Sampling
======================================================================

This example demonstrates a simplified RL training loop:

1. ğŸ² ROLLOUT: Sample completions from model using vLLM (fast!)
2. ğŸ’° REWARD:  Compute rewards for each completion
3. ğŸ”§ TRAIN:   Update policy using async forward-backward with rewards
4. ğŸ” REPEAT:  Multiple training iterations

Pattern inspired by: Megatron-Bridge rlhf_with_bridge.py

âœ“ Server is healthy: {'status': 'ok'}

Creating LoRA adapter...
âœ“ Created adapter: base_lora_e80b8307

======================================================================
TRAINING CONFIGURATION
======================================================================
  RL Iterations: 5
  Samples per iteration: 5
  Total samples: 25
  Prompts: 5
======================================================================


======================================================================
ITERATION 1/5
======================================================================

ğŸ² Sampling completions with vLLM (model: base_lora_e80b8307)...
  Sample 1: âœ“ Generated 695 chars
  Sample 2: âœ“ Generated 282 chars
  Sample 3: âœ“ Generated 599 chars
  Sample 4: âœ“ Generated 500 chars
  Sample 5: âœ“ Generated 568 chars

ğŸ’° Computing rewards for 5 samples...
  Sample 1: reward=0.585 (length=695)
  Sample 2: reward=0.525 (length=282)
  Sample 3: reward=0.491 (length=599)
  Sample 4: reward=0.508 (length=500)
  Sample 5: reward=0.437 (length=568)

ğŸ”§ Async RL training on 5 samples (KL coeff: 0.1)...
Submitting training jobs...
  âœ“ Submitted sample 1 (reward=0.585)
  âœ“ Submitted sample 2 (reward=0.525)
  âœ“ Submitted sample 3 (reward=0.491)
  âœ“ Submitted sample 4 (reward=0.508)
  âœ“ Submitted sample 5 (reward=0.437)

âœ“ All 5 training jobs submitted in 0.01s
Processing in parallel...

Training results:
  Sample 1: loss=2549.1416 (base=2549.1416, kl_penalty=0.0000), reward=0.585, kl=0.0000 âœ“
  Sample 2: loss=1880.2870 (base=1880.2870, kl_penalty=0.0000), reward=0.525, kl=0.0000 âœ“
  Sample 3: loss=1700.4369 (base=1700.4369, kl_penalty=0.0000), reward=0.491, kl=0.0000 âœ“
  Sample 4: loss=1381.1644 (base=1381.1644, kl_penalty=0.0000), reward=0.508, kl=0.0000 âœ“
  Sample 5: loss=1612.5884 (base=1612.5884, kl_penalty=0.0000), reward=0.437, kl=0.0000 âœ“

âœ“ RL training complete in 5.76s
  Average loss: 1824.7237
  Average KL div: 0.0000
  Success rate: 5/5

ğŸ“Š Iteration 1 Summary:
  Avg Reward: 0.509
  Avg Loss: 1824.7237
  Avg KL Div: 0.0000
  Time: 5.76s

======================================================================
ITERATION 2/5
======================================================================

ğŸ² Sampling completions with vLLM (model: base_lora_e80b8307)...
  Sample 1: âœ“ Generated 695 chars
  Sample 2: âœ“ Generated 282 chars
  Sample 3: âœ“ Generated 599 chars
  Sample 4: âœ“ Generated 500 chars
  Sample 5: âœ“ Generated 568 chars

ğŸ’° Computing rewards for 5 samples...
  Sample 1: reward=0.577 (length=695)
  Sample 2: reward=0.556 (length=282)
  Sample 3: reward=0.319 (length=599)
  Sample 4: reward=0.685 (length=500)
  Sample 5: reward=0.368 (length=568)

ğŸ”§ Async RL training on 5 samples (KL coeff: 0.1)...
Submitting training jobs...
  âœ“ Submitted sample 1 (reward=0.577)
  âœ“ Submitted sample 2 (reward=0.556)
  âœ“ Submitted sample 3 (reward=0.319)
  âœ“ Submitted sample 4 (reward=0.685)
  âœ“ Submitted sample 5 (reward=0.368)

âœ“ All 5 training jobs submitted in 0.02s
Processing in parallel...

Training results:
  Sample 1: loss=2070.9106 (base=2070.9106, kl_penalty=0.0000), reward=0.577, kl=0.0000 âœ“
  Sample 2: loss=1568.0437 (base=1568.0437, kl_penalty=0.0000), reward=0.556, kl=0.0000 âœ“
  Sample 3: loss=870.8827 (base=870.8827, kl_penalty=0.0000), reward=0.319, kl=0.0000 âœ“
  Sample 4: loss=1520.1637 (base=1520.1637, kl_penalty=0.0000), reward=0.685, kl=0.0000 âœ“
  Sample 5: loss=1066.9823 (base=1066.9823, kl_penalty=0.0000), reward=0.368, kl=0.0000 âœ“

âœ“ RL training complete in 1.58s
  Average loss: 1419.3966
  Average KL div: 0.0000
  Success rate: 5/5

ğŸ“Š Iteration 2 Summary:
  Avg Reward: 0.501
  Avg Loss: 1419.3966
  Avg KL Div: 0.0000
  Time: 1.58s

======================================================================
ITERATION 3/5
======================================================================

ğŸ² Sampling completions with vLLM (model: base_lora_e80b8307)...
  Sample 1: âœ“ Generated 695 chars
  Sample 2: âœ“ Generated 282 chars
  Sample 3: âœ“ Generated 599 chars
  Sample 4: âœ“ Generated 500 chars
  Sample 5: âœ“ Generated 568 chars

ğŸ’° Computing rewards for 5 samples...
  Sample 1: reward=0.650 (length=695)
  Sample 2: reward=0.601 (length=282)
  Sample 3: reward=0.300 (length=599)
  Sample 4: reward=0.587 (length=500)
  Sample 5: reward=0.440 (length=568)

ğŸ”§ Async RL training on 5 samples (KL coeff: 0.1)...
Submitting training jobs...
  âœ“ Submitted sample 1 (reward=0.650)
  âœ“ Submitted sample 2 (reward=0.601)
  âœ“ Submitted sample 3 (reward=0.300)
  âœ“ Submitted sample 4 (reward=0.587)
  âœ“ Submitted sample 5 (reward=0.440)

âœ“ All 5 training jobs submitted in 0.02s
Processing in parallel...

Training results:
  Sample 1: loss=1836.6273 (base=1836.6273, kl_penalty=0.0000), reward=0.650, kl=0.0000 âœ“
  Sample 2: loss=1301.4757 (base=1301.4757, kl_penalty=0.0000), reward=0.601, kl=0.0000 âœ“
  Sample 3: loss=619.7842 (base=619.7842, kl_penalty=0.0000), reward=0.300, kl=0.0000 âœ“
  Sample 4: loss=999.9420 (base=999.9420, kl_penalty=0.0000), reward=0.587, kl=0.0000 âœ“
  Sample 5: loss=977.6712 (base=977.6712, kl_penalty=0.0000), reward=0.440, kl=0.0000 âœ“

âœ“ RL training complete in 1.58s
  Average loss: 1147.1001
  Average KL div: 0.0000
  Success rate: 5/5

ğŸ“Š Iteration 3 Summary:
  Avg Reward: 0.516
  Avg Loss: 1147.1001
  Avg KL Div: 0.0000
  Time: 1.58s

======================================================================
ITERATION 4/5
======================================================================

ğŸ² Sampling completions with vLLM (model: base_lora_e80b8307)...
  Sample 1: âœ“ Generated 695 chars
  Sample 2: âœ“ Generated 282 chars
  Sample 3: âœ“ Generated 599 chars
  Sample 4: âœ“ Generated 500 chars
  Sample 5: âœ“ Generated 568 chars

ğŸ’° Computing rewards for 5 samples...
  Sample 1: reward=0.544 (length=695)
  Sample 2: reward=0.612 (length=282)
  Sample 3: reward=0.427 (length=599)
  Sample 4: reward=0.675 (length=500)
  Sample 5: reward=0.313 (length=568)

ğŸ”§ Async RL training on 5 samples (KL coeff: 0.1)...
Submitting training jobs...
  âœ“ Submitted sample 1 (reward=0.544)
  âœ“ Submitted sample 2 (reward=0.612)
  âœ“ Submitted sample 3 (reward=0.427)
  âœ“ Submitted sample 4 (reward=0.675)
  âœ“ Submitted sample 5 (reward=0.313)

âœ“ All 5 training jobs submitted in 0.02s
Processing in parallel...

Training results:
  Sample 1: loss=1100.3860 (base=1100.3860, kl_penalty=0.0000), reward=0.544, kl=0.0000 âœ“
  Sample 2: loss=950.8201 (base=950.8201, kl_penalty=0.0000), reward=0.612, kl=0.0000 âœ“
  Sample 3: loss=634.8282 (base=634.8282, kl_penalty=0.0000), reward=0.427, kl=0.0000 âœ“
  Sample 4: loss=849.8809 (base=849.8809, kl_penalty=0.0000), reward=0.675, kl=0.0000 âœ“
  Sample 5: loss=487.0111 (base=487.0111, kl_penalty=0.0000), reward=0.313, kl=0.0000 âœ“

âœ“ RL training complete in 1.57s
  Average loss: 804.5853
  Average KL div: 0.0000
  Success rate: 5/5

ğŸ“Š Iteration 4 Summary:
  Avg Reward: 0.514
  Avg Loss: 804.5853
  Avg KL Div: 0.0000
  Time: 1.57s

======================================================================
ITERATION 5/5
======================================================================

ğŸ² Sampling completions with vLLM (model: base_lora_e80b8307)...
  Sample 1: âœ“ Generated 695 chars
  Sample 2: âœ“ Generated 282 chars
  Sample 3: âœ“ Generated 599 chars
  Sample 4: âœ“ Generated 500 chars
  Sample 5: âœ“ Generated 568 chars

ğŸ’° Computing rewards for 5 samples...
  Sample 1: reward=0.534 (length=695)
  Sample 2: reward=0.617 (length=282)
  Sample 3: reward=0.359 (length=599)
  Sample 4: reward=0.570 (length=500)
  Sample 5: reward=0.484 (length=568)

ğŸ”§ Async RL training on 5 samples (KL coeff: 0.1)...
Submitting training jobs...
  âœ“ Submitted sample 1 (reward=0.534)
  âœ“ Submitted sample 2 (reward=0.617)
  âœ“ Submitted sample 3 (reward=0.359)
  âœ“ Submitted sample 4 (reward=0.570)
  âœ“ Submitted sample 5 (reward=0.484)

âœ“ All 5 training jobs submitted in 0.02s
Processing in parallel...

Training results:
  Sample 1: loss=744.6816 (base=744.6816, kl_penalty=0.0000), reward=0.534, kl=0.0000 âœ“
  Sample 2: loss=637.0995 (base=637.0995, kl_penalty=0.0000), reward=0.617, kl=0.0000 âœ“
  Sample 3: loss=343.1037 (base=343.1037, kl_penalty=0.0000), reward=0.359, kl=0.0000 âœ“
  Sample 4: loss=478.2317 (base=478.2317, kl_penalty=0.0000), reward=0.570, kl=0.0000 âœ“
  Sample 5: loss=519.3758 (base=519.3758, kl_penalty=0.0000), reward=0.484, kl=0.0000 âœ“

âœ“ RL training complete in 1.57s
  Average loss: 544.4985
  Average KL div: 0.0000
  Success rate: 5/5

ğŸ“Š Iteration 5 Summary:
  Avg Reward: 0.513
  Avg Loss: 544.4985
  Avg KL Div: 0.0000
  Time: 1.57s

======================================================================
FINAL SUMMARY
======================================================================

âœ“ RL Training Complete!

Model: base_lora_e80b8307
Total Iterations: 5
Total Samples Trained: 25
Total Training Time: 12.07s

PERFORMANCE METRICS:
  Overall Avg Reward: 0.511
  Overall Avg Loss: 1148.0608
  Throughput: 2.07 samples/sec
  Time per iteration: 2.41s

PATTERN DEMONSTRATED:
1. vLLM Policy Sampling (fast inference with LoRA)
2. Reward Computation (simple heuristic â†’ replace with reward model)
3. Async Training (parallel forward-backward with rewards)

NEXT STEPS FOR PRODUCTION RLHF:
- Use real reward model (sentiment, helpfulness, safety)
- Implement PPO loss instead of simple importance sampling
- Track old_log_probs for KL divergence constraint
- Use Bridge.export_hf_weights() to sync training â†’ inference
- See: examples/rl/rlhf_with_bridge.py in Megatron-Bridge

ADVANCED PATTERN (from rlhf_with_bridge.py):
- HF model for generation (vLLM or transformers)
- Reward model scores completions
- Megatron-Bridge trains policy with custom loss
- Bridge syncs weights back to HF model for next rollout

======================================================================
âœ“ Example complete!
======================================================================

shadeform@shadecloud:~/Tinkerbell/examples$ 